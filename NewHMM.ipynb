{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import math\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from create_train_test_val_maps import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_map_revived = open_map('/home/cs231n/data/train')\n",
    "val_map_revived = open_map('/home/cs231n/data/val')\n",
    "test_map_revived = open_map('/home/cs231n/data/test')\n",
    "#codes = [45021, 44004, 43004, 45008, 45002, 45007]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get global index from (row, col) index\n",
    "def sub2ind(array_shape, row, col):\n",
    "    ind = row*array_shape[1] + col\n",
    "    if row < 0 or row >= array_shape[0]:\n",
    "        ind = -1\n",
    "    if col < 0 or col >= array_shape[1]:\n",
    "        ind = -1\n",
    "    return ind\n",
    "\n",
    "# get (row, col) index from global index\n",
    "def ind2sub(array_shape, ind):\n",
    "    row = int(ind) / array_shape[1]\n",
    "    col = ind % array_shape[1]\n",
    "    if ind < 0:\n",
    "        row = -1\n",
    "        col = -1\n",
    "    if ind >=  array_shape[0]*array_shape[1]:\n",
    "        row = -1\n",
    "        col = -1\n",
    "    return (row, col)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax function for input. \n",
    "    Use tricks from previous assignment to avoid overflow\n",
    "    \"\"\"\n",
    "\t### YOUR CODE HERE\n",
    "    xshift = np.max(x, axis = 1)\n",
    "    xshift = xshift.reshape((x.shape[0],1))\n",
    "    x = x - xshift\n",
    "    s = np.exp(x) / np.sum(np.exp(x),axis = 1).reshape((x.shape[0],1))\n",
    "\t### END YOUR CODE\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Categorical Data to Numeric Format\n",
    "\n",
    "# Train Data\n",
    "category_var = ['Veh Ref ID','Event Type Description','Brake Switch','Clutch Switch','Cruise Status','Dpf Regen Inhibit Sw', \n",
    "                'Dpf Thermal Mngmnt','Eng Coolant Level','DTCID']\n",
    "for vehicleID in train_map_revived.keys():\n",
    "    for ATA6code in train_map_revived[vehicleID].keys():\n",
    "        for time_window in range(len(train_map_revived[vehicleID][ATA6code].keys())):\n",
    "            for i in range(len(train_map_revived[vehicleID][ATA6code][time_window])):\n",
    "                for c in category_var:\n",
    "                    train_map_revived[vehicleID][ATA6code][time_window][i][c] = train_map_revived[vehicleID][ATA6code][time_window][i][c].astype('category')\n",
    "                    train_map_revived[vehicleID][ATA6code][time_window][i][c] = train_map_revived[vehicleID][ATA6code][time_window][i][c].cat.codes\n",
    "\n",
    "# Validate Data\n",
    "for vehicleID in val_map_revived.keys():\n",
    "    for ATA6code in val_map_revived[vehicleID].keys():\n",
    "        for time_window in range(len(val_map_revived[vehicleID][ATA6code].keys())):\n",
    "            for i in range(len(val_map_revived[vehicleID][ATA6code][time_window])):\n",
    "                for c in category_var:\n",
    "                    val_map_revived[vehicleID][ATA6code][time_window][i][c] = val_map_revived[vehicleID][ATA6code][time_window][i][c].astype('category')\n",
    "                    val_map_revived[vehicleID][ATA6code][time_window][i][c] = val_map_revived[vehicleID][ATA6code][time_window][i][c].cat.codes\n",
    "\n",
    "\n",
    "# Test Data\n",
    "for vehicleID in test_map_revived.keys():\n",
    "    for ATA6code in test_map_revived[vehicleID].keys():\n",
    "        for time_window in range(len(test_map_revived[vehicleID][ATA6code].keys())):\n",
    "            for i in range(len(test_map_revived[vehicleID][ATA6code][time_window])):\n",
    "                for c in category_var:\n",
    "                    test_map_revived[vehicleID][ATA6code][time_window][i][c] = test_map_revived[vehicleID][ATA6code][time_window][i][c].astype('category')\n",
    "                    test_map_revived[vehicleID][ATA6code][time_window][i][c] = test_map_revived[vehicleID][ATA6code][time_window][i][c].cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "num_states = 2\n",
    "num_codes = len(codes)\n",
    "num_time_windows = 10\n",
    "num_iter = 100\n",
    "models = {}\n",
    "\n",
    "codes = []\n",
    "code_window_map = {}\n",
    "for vehicleID in train_map_revived.keys():\n",
    "    for ATA6code in train_map_revived[vehicleID].keys():\n",
    "        if ATA6code not in codes:\n",
    "            codes.append(ATA6code)\n",
    "        if ATA6code not in code_window_map:\n",
    "            code_window_map[ATA6code] = {}\n",
    "        for time_window in train_map_revived[vehicleID][ATA6code].keys():\n",
    "            if time_window not in code_window_map[ATA6code]:\n",
    "                code_window_map[ATA6code][time_window] = []\n",
    "            for sequence_of_snapshots in train_map_revived[vehicleID][ATA6code][time_window]:\n",
    "                code_window_map[ATA6code][time_window].append(sequence_of_snapshots)\n",
    "                \n",
    "for ATA6code in code_window_map.keys():\n",
    "    for time_window in code_window_map[ATA6code].keys():\n",
    "        lengths = []\n",
    "        listofsequences = code_window_map[ATA6code][time_window]\n",
    "        X = pandas.concat(listofsequences).as_matrix()[:,3:] \n",
    "        for sequence in listofsequences:\n",
    "            lengths.append(sequence.shape[0]) \n",
    "        models[(ATA6code, time_window)] = hmm.GaussianHMM(n_components=num_states, n_iter=num_iter).fit(X,lengths)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11 5\n",
      "0 1 6\n",
      "0 18 3\n",
      "0 5 8\n",
      "0 1 0\n",
      "0 19 9\n",
      "0 26 8\n",
      "0 5 5\n",
      "0 7 4\n",
      "0 27 9\n",
      "0 29 9\n",
      "0 23 4\n",
      "0 24 4\n",
      "0 27 4\n",
      "0 10 4\n",
      "0 29 6\n",
      "0 12 2\n",
      "0 32 7\n",
      "0 8 0\n",
      "0 20 1\n",
      "0 0 6\n",
      "0 17 7\n",
      "0 9 5\n",
      "0 13 5\n",
      "0 15 5\n",
      "0 17 8\n",
      "0 4 4\n",
      "0 15 8\n",
      "0 28 3\n",
      "0 8 9\n",
      "0 7 3\n",
      "0 28 6\n",
      "0 8 4\n",
      "0 24 9\n",
      "0 10 7\n",
      "0 30 3\n",
      "0 25 8\n",
      "0 32 4\n",
      "0 16 2\n",
      "0 19 1\n",
      "0 12 1\n",
      "0 17 2\n",
      "0 26 2\n",
      "0 15 2\n",
      "0 4 6\n",
      "0 31 6\n",
      "0 11 4\n",
      "0 13 8\n",
      "0 5 2\n",
      "0 12 8\n",
      "0 28 0\n",
      "0 21 8\n",
      "0 24 3\n",
      "0 1 3\n",
      "0 21 4\n",
      "0 25 6\n",
      "0 11 2\n",
      "0 10 2\n",
      "0 3 5\n",
      "0 5 9\n",
      "0 8 7\n",
      "0 6 1\n",
      "0 19 2\n",
      "0 14 5\n",
      "0 17 1\n",
      "0 31 8\n",
      "0 23 5\n",
      "0 21 6\n",
      "0 1 4\n",
      "0 4 5\n",
      "0 26 4\n",
      "0 5 1\n",
      "0 7 8\n",
      "0 4 1\n",
      "0 23 0\n",
      "0 24 0\n",
      "0 30 0\n",
      "0 28 5\n",
      "0 10 8\n",
      "0 2 5\n",
      "0 29 2\n",
      "0 25 1\n",
      "0 9 1\n",
      "0 16 5\n",
      "0 20 5\n",
      "0 3 0\n",
      "0 0 2\n",
      "0 16 8\n",
      "0 7 1\n",
      "0 19 7\n",
      "0 14 0\n",
      "0 7 5\n",
      "0 2 1\n",
      "0 6 2\n",
      "0 19 5\n",
      "0 5 6\n",
      "0 1 1\n",
      "0 26 9\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "rows of transmat_ must sum to 1.0 (got [ 1.  0.])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-27014cb49a20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m                     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                     \u001b[0mlog_likelihoods\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/hmmlearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, lengths)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \"\"\"\n\u001b[0;32m    240\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"startprob_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/hmmlearn/hmm.py\u001b[0m in \u001b[0;36m_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGaussianHMM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeans_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeans_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/hmmlearn/base.py\u001b[0m in \u001b[0;36m_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    522\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransmat_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             raise ValueError(\"rows of transmat_ must sum to 1.0 (got {0})\"\n\u001b[1;32m--> 524\u001b[1;33m                              .format(self.transmat_.sum(axis=1)))\n\u001b[0m\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_compute_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: rows of transmat_ must sum to 1.0 (got [ 1.  0.])"
     ]
    }
   ],
   "source": [
    "# VALIDATE\n",
    "lengths = []\n",
    "for vehicleID in val_map_revived.keys():\n",
    "    for ATA6code in val_map_revived[vehicleID].keys():\n",
    "        for time_window in val_map_revived[vehicleID][ATA6code].keys():\n",
    "            for sequence_of_snapshots in val_map_revived[vehicleID][ATA6code][time_window]:\n",
    "                lengths.append(sequence_of_snapshots.shape[0])\n",
    "num_val = sum(lengths)\n",
    "\n",
    "sample = 0\n",
    "labels = []\n",
    "log_likelihoods = np.zeros((num_val, num_codes, num_time_windows))\n",
    "for vehicleID in val_map_revived.keys():\n",
    "    for ATA6code in val_map_revived[vehicleID].keys():\n",
    "        for time_window in val_map_revived[vehicleID][ATA6code].keys():\n",
    "            for sequence_of_snapshots in val_map_revived[vehicleID][ATA6code][time_window]:\n",
    "                labels.append([codes.index(ATA6code),time_window])\n",
    "                x = sequence_of_snapshots.as_matrix()[:,3:] \n",
    "                for pair in models.keys():\n",
    "                    c = codes.index(pair[0])\n",
    "                    w = pair[1]\n",
    "                    print(sample,c,w)\n",
    "                    log_likelihoods[sample,c,w] = models[pair].score(x)\n",
    "        sample = sample+1                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#accuracy\n",
    "errors = np.zeros((labels.shape))\n",
    "correct_codes = np.zeros((labels.shape))\n",
    "probabilities = np.zeros(log_likelihoods.shape)\n",
    "time_counts = 0\n",
    "for s in range(0,num_val):\n",
    "    probabilities_temp = softmax(log_likelihoods[s,:,:].reshape(1,num_codes*num_time_steps))\n",
    "    probabilities[s] = probabilities_temp.reshape(num_codes, num_time_steps) \n",
    "    max_index = np.unravel_index(probabilities[s].argmax(), probabilities[s].shape)\n",
    "    errors[s] = max_index - np.array([codes.index(labels[s,0]),labels[s,1]])\n",
    "    correct_codes[s] = max_index == np.array([codes.index(labels[s,0]),labels[s,1]])\n",
    "    if correct_codes[s,0] == True and correct_codes[s,1] == True:\n",
    "        time_counts = time_counts + 1        \n",
    "error_sum = sum(errors)\n",
    "correct_sum = sum(correct_codes)\n",
    "code_accuracy = correct_sum[0]/num_val\n",
    "k_accuracy = time_counts/num_val\n",
    "mse = (1.0/num_val)*sum(np.power(errors[:,0],2))\n",
    "\n",
    "\n",
    "print('ATA6: Mean Squared Error',mse)\n",
    "print('ATA6: Accuracy', code_accuracy)\n",
    "print('K: Accuracy', k_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "code_probabilities = np.zeros((num_val, num_codes))\n",
    "predicted_codes = np.zeros(num_val)\n",
    "for s in range(0,num_val):\n",
    "    probabilities_temp = softmax(log_likelihoods[s,:,:].reshape(1,num_codes*num_time_steps))\n",
    "    code_probabilities[s,:] = np.sum(probabilities_temp.reshape(num_codes, num_time_steps),axis=1)\n",
    "    predicted_codes[s] = np.argmax(code_probabilities[s,:])\n",
    "code_accuracy = np.sum(predicted_codes == labels[:,0])/num_val\n",
    "print(\"Marginalized Predicted Code Accuracy: \", code_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "codes = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(1 not in codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
