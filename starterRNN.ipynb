{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import print_function\n",
    "from create_train_test_val_maps import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# get global index from (row, col) index\n",
    "def sub2ind(array_shape, row, col):\n",
    "    ind = row*array_shape[1] + col\n",
    "    if row < 0 or row >= array_shape[0]:\n",
    "        ind = -1\n",
    "    if col < 0 or col >= array_shape[1]:\n",
    "        ind = -1\n",
    "    return ind\n",
    "\n",
    "# get (row, col) index from global index\n",
    "def ind2sub(array_shape, ind):\n",
    "    row = int(ind) / array_shape[1]\n",
    "    col = ind % array_shape[1]\n",
    "    if ind < 0:\n",
    "        row = -1\n",
    "        col = -1\n",
    "    if ind >=  array_shape[0]*array_shape[1]:\n",
    "        row = -1\n",
    "        col = -1\n",
    "    return (row, col)\n",
    "\n",
    "def one_hot_labels(labels, num_classes):\n",
    "    one_hot_labels = np.zeros((labels.size, num_classes))\n",
    "    one_hot_labels[np.arange(labels.size),labels.astype(int)] = 1\n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_windows = 5\n",
    "window_size = 20\n",
    "selected_codes = [0, 45021, 44004, 43004, 45008, 45002, 45007]\n",
    "\n",
    "train_combined, val_combined, test_combined = load_maps(num_windows, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(selected_codes, data_map, num_windows):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for vehicleID in data_map.keys():\n",
    "        for ATA6code in data_map[vehicleID].keys():\n",
    "            if ATA6code not in selected_codes:\n",
    "                continue\n",
    "            for window in data_map[vehicleID][ATA6code].keys():\n",
    "                for sequence in data_map[vehicleID][ATA6code][window]:\n",
    "                    # Append only code label\n",
    "                    Y.append(selected_codes.index(ATA6code))\n",
    "                    # Append mapped label (code, window) --> (int)\n",
    "                    #Y.append(sub2ind((len(selected_codes),num_windows),selected_codes.index(ATA6code), window))\n",
    "                    X.append(sequence.as_matrix()) \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (number of samples) x (number of time steps) x (number of features)\n",
      "train_fieldsnaps shape:  (946, 20, 61)\n",
      "validation_fieldsnaps shape:  (197, 20, 61)\n",
      "test_fieldsnaps shape:  (137, 20, 61)\n",
      "train_results.shape:  (946,)\n",
      "validation_results.shape:  (197,)\n",
      "test_results.shape:  (137,)\n",
      "one_hot_train_results.shape:  (946, 7)\n",
      "one_hot_validation_results.shape:  (197, 7)\n",
      "one_hot_test_results.shape:  (137, 7)\n"
     ]
    }
   ],
   "source": [
    "num_codes = len(selected_codes)\n",
    "\n",
    "# Get data\n",
    "train_fieldsnaps,train_results = get_data(selected_codes, train_combined, num_windows)\n",
    "validation_fieldsnaps,validation_results = get_data(selected_codes, val_combined, num_windows)\n",
    "test_fieldsnaps,test_results = get_data(selected_codes, test_combined, num_windows)\n",
    "\n",
    "# Turn into One Hot Labels\n",
    "#one_hot_train_results = one_hot_labels(train_results, num_codes*num_windows)\n",
    "#one_hot_validation_results = one_hot_labels(validation_results, num_codes*num_windows)\n",
    "#one_hot_test_results = one_hot_labels(test_results, num_codes*num_windows)\n",
    "\n",
    "one_hot_train_results = one_hot_labels(train_results, num_codes)\n",
    "one_hot_validation_results = one_hot_labels(validation_results, num_codes)\n",
    "one_hot_test_results = one_hot_labels(test_results, num_codes)\n",
    "\n",
    "print('shape: (number of samples) x (number of time steps) x (number of features)')\n",
    "print(\"train_fieldsnaps shape: \", train_fieldsnaps.shape)\n",
    "print(\"validation_fieldsnaps shape: \", validation_fieldsnaps.shape)\n",
    "print(\"test_fieldsnaps shape: \", test_fieldsnaps.shape)\n",
    "\n",
    "print(\"train_results.shape: \", train_results.shape)\n",
    "print(\"validation_results.shape: \", validation_results.shape)\n",
    "print(\"test_results.shape: \", test_results.shape)\n",
    "\n",
    "print(\"one_hot_train_results.shape: \", one_hot_train_results.shape)\n",
    "print(\"one_hot_validation_results.shape: \", one_hot_validation_results.shape)\n",
    "print(\"one_hot_test_results.shape: \", one_hot_test_results.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of validation samples:  197\n",
      "number of non repairs in validation set:  93\n",
      "percent of non repairs in validation set:  0.4720812182741117\n",
      "number of test samples:  137\n",
      "number of non repairs in test set:  69\n",
      "percent of non repairs in test set:  0.5036496350364964\n"
     ]
    }
   ],
   "source": [
    "#print(np.count_nonzero(train_results))\n",
    "print('number of validation samples: ', len(validation_results))\n",
    "print('number of non repairs in validation set: ',len(validation_results) - np.count_nonzero(validation_results))\n",
    "print('percent of non repairs in validation set: ',(len(validation_results) - np.count_nonzero(validation_results))/len(validation_results))\n",
    "#print(np.count_nonzero(test_results))\n",
    "print('number of test samples: ',len(test_results))\n",
    "print('number of non repairs in test set: ',len(test_results) - np.count_nonzero(test_results))\n",
    "print('percent of non repairs in test set: ',(len(test_results) - np.count_nonzero(test_results))/len(test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(x, W, b, num_hidden, timesteps):\n",
    "    \n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # data shape: (batch_size, timesteps, num_input)\n",
    "    # Each Time Step Required shape: (batch_size, num_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # lstm layer\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # lstm out\n",
    "    x_out, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # linear forward propogation \n",
    "    return tf.matmul(x_out[-1], W['out']) + b['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train function sets up an LSTM neural net and trains\n",
    "def train_RNN(save_file, \n",
    "              learning_rate,\n",
    "              lambda_,\n",
    "              num_hidden,\n",
    "              num_classes,\n",
    "              num_epochs, \n",
    "              iters_per_epoch, \n",
    "              batch_size, \n",
    "              timesteps, \n",
    "              num_input, \n",
    "              window_size, \n",
    "              num_windows,\n",
    "              num_codes,\n",
    "              train_X, \n",
    "              train_Y, \n",
    "              val_X, \n",
    "              val_Y):    \n",
    "        \n",
    "    # Set up Graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Graph Input\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "    }\n",
    "\n",
    "    #\n",
    "    logits = RNN(X, weights, biases, num_hidden, timesteps)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=Y)) \n",
    "    regularizer = tf.nn.l2_loss(weights['out'])\n",
    "    loss = tf.reduce_mean(loss + lambda_*regularizer)\n",
    "\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "    # Evaluate model (with test logits, for dropout to be disabled)\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize variables \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # TRAIN ----------------------------------\n",
    "    saver = tf.train.Saver()\n",
    "    # arrays to hold losses and accuracies for each epoch\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "\n",
    "        # Start training\n",
    "        for epoch in range(0,num_epochs):\n",
    "            for iter_ in range(0, iters_per_epoch):\n",
    "                #print(iter_)\n",
    "                if (iter_ + batch_size) > train_fieldsnaps.shape[0]:\n",
    "                    continue\n",
    "                batch_x = train_fieldsnaps[iter_:iter_ + batch_size]\n",
    "                batch_y = one_hot_train_results[iter_:iter_ + batch_size]\n",
    "                batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "            train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: train_X, Y: train_Y})\n",
    "            val_loss, val_acc = sess.run([loss, accuracy], feed_dict={X: val_X, Y: val_Y})\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            # print accuracy and loss for each epoch\n",
    "            print(\"\\nEpoch \" + str(epoch) + '\\n-------------------\\n' + \"Training Loss= \" + \\\n",
    "                  \"{:.4f}\".format(train_loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(train_acc) + '\\n' + \"Validation Loss= \" + \\\n",
    "                 \"{:.4f}\".format(val_loss) + \", Validation Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(val_acc))\n",
    "        print(\"\\nOptimization Finished!\")\n",
    "        # save model into save_file\n",
    "        saver.save(sess, save_file)\n",
    "        return train_losses, train_accs, val_losses, val_accs\n",
    "    \n",
    "# test a saved model on a given test data  \n",
    "def test_rnn(save_file,\n",
    "             learning_rate,\n",
    "             lambda_,\n",
    "             num_hidden,\n",
    "             num_classes,\n",
    "             batch_size, \n",
    "              timesteps, \n",
    "              num_input, \n",
    "              window_size, \n",
    "              num_windows, \n",
    "              num_codes,\n",
    "              test_X, \n",
    "              test_Y):\n",
    "        \n",
    "    # Set Up The Same Graph\n",
    "    #-------------------------\n",
    "    # Training Parameters\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes]) # num_classes\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "    }\n",
    "    \n",
    "    # Call RNN setup and softmax\n",
    "    logits = RNN(X, weights, biases, num_hidden, timesteps)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Loss Function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=Y)) \n",
    "    regularizer = tf.nn.l2_loss(weights['out'])\n",
    "    loss = tf.reduce_mean(loss + lambda_*regularizer)\n",
    "\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "    # Evaluate accuracy for model (disable dropouts)\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Get weights from saved model\n",
    "    saver = tf.train.Saver() \n",
    "\n",
    "    # Open model from saved file and test accuracy\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, './' + save_file)\n",
    "        #test_accuracy = sess.run(accuracy, feed_dict={X: test_X, Y: test_Y})\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_X, Y: test_Y})\n",
    "    return test_accuracy\n",
    "\n",
    "# prints 2 plots, one for training and one for loss for both train and loss\n",
    "def plot_results(num_epochs, train_losses, train_accs, val_losses, val_accs):\n",
    "    plot_training_accuracy = plt.plot(np.arange(0,num_epochs),train_accs ,label='Training')\n",
    "    plot_val_accuracy = plt.plot(np.arange(0,num_epochs),val_accs,label='Validation')\n",
    "    ax = plt.gca() # grab the current axis\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.suptitle(\"\")\n",
    "    plt.title('Training vs Validation Accuracy')\n",
    "    mylegend = plt.legend(loc=\"right corner\",bbox_to_anchor=(1,1))\n",
    "    plt.rcParams[\"figure.figsize\"] = [6,5]\n",
    "    plt.savefig('Training vs Validation Accuracy'.replace(\" \", \"_\"), bbox_extra_artists=(mylegend,), bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    plot_training_accuracy = plt.plot(np.arange(0,num_epochs),train_losses ,label='Training')\n",
    "    plot_val_accuracy = plt.plot(np.arange(0,num_epochs),val_losses,label='Validation')\n",
    "    ax = plt.gca() # grab the current axis\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.suptitle(\"\")\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    mylegend = plt.legend(loc=\"right corner\")\n",
    "    plt.savefig('Training vs Validation Accuracy'.replace(\" \", \"_\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n",
      "-------------------\n",
      "Training Loss= 9.6354, Training Accuracy= 0.260\n",
      "Validation Loss= 10.0823, Validation Accuracy= 0.234\n",
      "\n",
      "Epoch 1\n",
      "-------------------\n",
      "Training Loss= 7.0910, Training Accuracy= 0.253\n",
      "Validation Loss= 8.0282, Validation Accuracy= 0.249\n",
      "\n",
      "Epoch 2\n",
      "-------------------\n",
      "Training Loss= 7.3860, Training Accuracy= 0.516\n",
      "Validation Loss= 8.3813, Validation Accuracy= 0.381\n",
      "\n",
      "Epoch 3\n",
      "-------------------\n",
      "Training Loss= 7.4056, Training Accuracy= 0.511\n",
      "Validation Loss= 8.3830, Validation Accuracy= 0.376\n",
      "\n",
      "Epoch 4\n",
      "-------------------\n",
      "Training Loss= 7.3764, Training Accuracy= 0.531\n",
      "Validation Loss= 8.4841, Validation Accuracy= 0.396\n",
      "\n",
      "Epoch 5\n",
      "-------------------\n",
      "Training Loss= 7.4349, Training Accuracy= 0.536\n",
      "Validation Loss= 8.5254, Validation Accuracy= 0.391\n",
      "\n",
      "Epoch 6\n",
      "-------------------\n",
      "Training Loss= 7.5059, Training Accuracy= 0.530\n",
      "Validation Loss= 8.6026, Validation Accuracy= 0.386\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e7bc9abebd8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m                                                            \u001b[0mone_hot_train_results\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                                                            \u001b[0mvalidation_fieldsnaps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                                                            one_hot_validation_results)\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;31m# test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m accuracy = test_rnn(save_file,\n",
      "\u001b[1;32m<ipython-input-11-d32d8a8f8409>\u001b[0m in \u001b[0;36mtrain_RNN\u001b[1;34m(save_file, learning_rate, lambda_, num_hidden, num_classes, num_epochs, iters_per_epoch, batch_size, timesteps, num_input, window_size, num_windows, num_codes, train_X, train_Y, val_X, val_Y)\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mone_hot_train_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miter_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0miter_\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[0mbatch_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m                 \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval_Y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 30 \n",
    "num_classes = num_codes\n",
    "num_input = 61          # number of features\n",
    "timesteps = window_size # number of terms in a sequence\n",
    "learning_rate = 0.001   # learning rate\n",
    "lambda_ = 0.01          # regularization\n",
    "num_hidden = 139        # number of hidden units\n",
    "#num_classes = num_codes*num_windows \n",
    " \n",
    "save_file = 'train_model'  + str(1) + '.ckpt'\n",
    "\n",
    "# (10,10), xxx samples, batch_size = 28, iters_per_epoch = 68\n",
    "# ( 5,20), 946 samples, batch_size = 11, iters_per_epoch = 86\n",
    "#  (5,50), xxx samples, batch_size = 29, iters_per_epoch = 29\n",
    "# (5,100), xxx samples, batch_size = 15, iters_per_epoch = 47\n",
    "# (1,50),  xxx samples, batch_size = xx, iters_per_epoch = xx\n",
    "batch_size = 11 \n",
    "iters_per_epoch = 86\n",
    "\n",
    "# train\n",
    "train_losses, train_accs, val_losses, val_accs = train_RNN(save_file, \n",
    "                                                           learning_rate,\n",
    "                                                           lambda_,\n",
    "                                                           num_hidden,\n",
    "                                                           num_classes,\n",
    "                                                           num_epochs, \n",
    "                                                           iters_per_epoch, \n",
    "                                                           batch_size, \n",
    "                                                           timesteps, \n",
    "                                                           num_input, \n",
    "                                                           window_size,\n",
    "                                                           num_windows,\n",
    "                                                           num_codes,\n",
    "                                                           train_fieldsnaps, \n",
    "                                                           one_hot_train_results, \n",
    "                                                           validation_fieldsnaps, \n",
    "                                                           one_hot_validation_results)\n",
    "# test\n",
    "accuracy = test_rnn(save_file,\n",
    "              learning_rate,\n",
    "              lambda_,\n",
    "              num_hidden,\n",
    "              num_classes,\n",
    "              batch_size, \n",
    "              timesteps, \n",
    "              num_input, \n",
    "              window_size, \n",
    "              num_windows, \n",
    "              num_codes,\n",
    "              test_fieldsnaps, \n",
    "              one_hot_test_results)\n",
    "\n",
    "print('\\nAccuracy', accuracy)\n",
    "plot_results(num_epochs, train_losses, train_accs, val_losses, val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,192):\n",
    "    if 192%i == 0:\n",
    "        print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
